{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humpback Whale Identifaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Data and Primitive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Datas/train.csv')\n",
    "train_df.head()\n",
    "train_images = glob(\"Datas/train/*jpg\")\n",
    "test_images = glob(\"../input/test/*jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Image\"] = train_df[\"Image\"].map( lambda x : \"Datas/train/\"+x)\n",
    "ImageToLabelDict = dict(zip(train_df[\"Image\"], train_df[\"Id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 100\n",
    "#image are imported and resized\n",
    "def ImportImage( filename):\n",
    "    img = Image.open(filename).resize( (SIZE,SIZE))\n",
    "    img = np.array(img)\n",
    "    if img.ndim == 2: #imported BW picture and converting to \"dumb RGB\"\n",
    "        img = np.tile( img, (3,1,1)).transpose((1,2,0))\n",
    "    return img\n",
    "x_train = np.array([ImportImage(img) for img in train_images])\n",
    "x = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelOneHotEncoder():\n",
    "    def __init__(self):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.le = LabelEncoder()\n",
    "    def fit_transform(self, x):\n",
    "        features = self.le.fit_transform( x)\n",
    "        return self.ohe.fit_transform( features.reshape(-1,1))\n",
    "    def transform( self, x):\n",
    "        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))\n",
    "    def inverse_tranform( self, x):\n",
    "        return self.le.inverse_transform( self.ohe.inverse_tranform( x))\n",
    "    def inverse_labels( self, x):\n",
    "        return self.le.inverse_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(ImageToLabelDict.get, train_images))\n",
    "lohe = LabelOneHotEncoder()\n",
    "y_cat = lohe.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of an image generator for preprocessing and data augmentation\n",
    "x_train = x_train.reshape( (-1,SIZE,SIZE,3))\n",
    "input_shape = x_train[0].shape\n",
    "#x_train = x_train.astype(\"float32\")\n",
    "y_train = y_cat\n",
    "\n",
    "image_gen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "\trescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "#training the image preprocessing\n",
    "image_gen.fit(x_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Data Tackling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr, n_images=4):\n",
    "    fig, axes = plt.subplots(n_images, n_images, figsize=(12,12))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        if img.ndim != 2:\n",
    "            img = img.reshape((SIZE,SIZE))\n",
    "        ax.imshow( img, cmap=\"Greys_r\")\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image augmentation with Keras prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of an image generator for preprocessing and data augmentation\n",
    "x = x.reshape((-1,SIZE,SIZE,1))\n",
    "x_train = x.astype(\"float32\")\n",
    "y_train = y_cat\n",
    "\n",
    "image_gen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "#training the image preprocessing\n",
    "image_gen.fit(x_train, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape( (-1,SIZE,SIZE,3))\n",
    "input_shape = x_train[0].shape\n",
    "#x_train = x_train.astype(\"float32\")\n",
    "y_train = y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d675a854873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'shap'"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_classes = len(y_cat.toarray()[0])\n",
    "epochs = 10\n",
    "input_shape = x_train[0].shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 3, 3, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 48)                221232    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4251)              208299    \n",
      "=================================================================\n",
      "Total params: 15,146,571\n",
      "Trainable params: 2,791,691\n",
      "Non-trainable params: 12,354,880\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/royn/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Epoch 1/10\n",
      " 91/985 [=>............................] - ETA: 24:25 - loss: 8.2630 - acc: 0.0680"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#picking vgg16 as pretrained (base) model https://keras.io/applications/#vgg16\n",
    "conv_base = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#maybe unfreeze last layer\n",
    "conv_base.layers[-2].trainable = True\n",
    "\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(48, activation='relu')) #64\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(48, activation='relu')) #48\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n",
    "          steps_per_epoch=x_train.shape[0] // epochs,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=[TensorBoard(log_dir='./tmp/log', write_graph=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 197x197; got `input_shape=(100, 100, 3)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-de72e8735b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#picking vgg16 as pretrained (base) model https://keras.io/applications/#vgg16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconv_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/applications/resnet50.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    296\u001b[0m                     raise ValueError('Input size must be at least ' +\n\u001b[1;32m    297\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'; got '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                                      '`input_shape=' + str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input size must be at least 197x197; got `input_shape=(100, 100, 3)`"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#picking vgg16 as pretrained (base) model https://keras.io/applications/#vgg16\n",
    "conv_base = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#maybe unfreeze last layer\n",
    "conv_base.layers[-2].trainable = True\n",
    "\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(48, activation='relu')) #64\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(48, activation='relu')) #48\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n",
    "          steps_per_epoch=x_train.shape[0] // epochs,\n",
    "          epochs=epochs,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
